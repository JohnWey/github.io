<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JohnWey&#39;s Blog</title>
  
  <subtitle>终身学习</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-09-26T09:24:30.104Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Wei</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习第一周总结</title>
    <link href="http://yoursite.com/2017/09/26/ML-Linear%20Regression/"/>
    <id>http://yoursite.com/2017/09/26/ML-Linear Regression/</id>
    <published>2017-09-26T09:24:30.104Z</published>
    <updated>2017-09-26T09:24:30.104Z</updated>
    
    <content type="html"><![CDATA[<p>先搞清楚什么是机器学习，这对定位问题是否应该使用机器学习来解决很重要，有些问题完全没必要使用机器学习，就没必要杀鸡用牛刀了。</p><hr><h4><strong>什么是机器学习？</strong></h4><ul><li><em>Arthur Samuel(1959):</em></li></ul><blockquote><p>Field of study that gives computers the ability to learn without being explicitly programmed.<strong>说人话：不需要太多的编程就能使计算机拥有学习某一领域的能力。</strong></p></blockquote><ul><li><em>Tom Mitchell(1998):</em></li></ul><blockquote><p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.<strong>说人话：AlphaGo为了完成人机对战（下棋 T），不断学习下棋，从中获取经验（E），目的是为了提高和人比赛的胜率（P）。</strong></p></blockquote><hr><h4><strong>机器学习分类</strong></h4><p>机器学习涉及的范围很广，针对不同的问题，学习策略也很多，但总体而言大致可以分为<strong>监督学习</strong>和<strong>非监督学习</strong>两种（还有半监督学习和强化学习）。不同的问题需要使用不同的策略。</p><p>还是把这两种策略的含义搞清楚先：</p><h5><strong>什么是监督学习？</strong>#####</h5><blockquote><p>可以理解为<strong>一部分数据的答案已经知道了</strong>。比如我们要预测未来大盘的点位，在历史中大盘的点位已经知道了；再比如我们要让机器知道给它的图片是个帅哥还是美女，前提是我们已经知道了这个图片是帅哥还是美女。</p></blockquote><p>可想而知，如果需要人工去分类（打标），是一个多大的工程。这也孕育出了很多以出售打标数据盈利的公司。</p><h5><strong>什么是无监督学习？</strong>#####</h5><blockquote><p>和监督学习相反，<strong>数据的答案事先我们不知道</strong>。而寻找答案是一个无中生有的过程。比如你遇到一群外星人，这群外星人各自有着不同的特征，你需要通过聚类的方法把有相同特征的外星人分组到一起，然后研究他们哪些对人类友好，哪些对人类有威胁，这叫无监督学习。</p></blockquote><p><em>算法一览：</em></p><table><thead><tr><th style="text-align:left">机器学习种类</th><th style="text-align:left">算法分类</th><th style="text-align:left">算法</th></tr></thead><tbody><tr><td style="text-align:left">监督学习</td><td style="text-align:left">分类、 回归</td><td style="text-align:left">K近邻、朴素贝叶斯、决策树、随机森林、GBDT和支持向量机；     线性回归、逻辑回归</td></tr><tr><td style="text-align:left">无监督学习</td><td style="text-align:left">聚类、 推荐</td><td style="text-align:left">K-Means、DBSCAN、协同过滤</td></tr><tr><td style="text-align:left">半监督学习</td><td style="text-align:left">聚类、 推荐</td><td style="text-align:left">标签传播</td></tr><tr><td style="text-align:left">强化学习</td><td style="text-align:left"></td><td style="text-align:left">隐马尔可夫</td></tr></tbody></table><hr><h4><strong>1. 能预测未来的神奇算法——线性回归</strong>####</h4><p>说到线性回归，其实我们初中的时候就学过它的简单方程式，只不过那会儿我们没有安利这样一个高大上的名字，我们那会儿叫<strong>斜截公式</strong>：$$y = kx + b$$来看看只有一个参数的<strong>单参数线性回归模型</strong>：$$h_\theta(x^i)=\theta_0+\theta_1 x_1^i  (其中 x_0^i=1，x_1^i表示一个特征，这个特征有i个值 )$$所谓的特征就是二维表中具有计算意义的一列数据，比如</p><table><thead><tr><th style="text-align:left">ID</th><th style="text-align:left">Sex</th><th style="text-align:left">High</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:left">男</td><td style="text-align:left">170 cm</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left">女</td><td style="text-align:left">175 cm</td></tr><tr><td style="text-align:left">3</td><td style="text-align:left">男</td><td style="text-align:left">180 cm</td></tr><tr><td style="text-align:left">4</td><td style="text-align:left">女</td><td style="text-align:left">200 cm</td></tr></tbody></table><p>其中Sex是一个特征，High是另一个特征，ID不是个特征，它只是个序列索引而已。他们的i都是4，因为有4条数据。是不是秒懂？：） 是的，在一元线性回归算法中，我们就是用一条倾斜的直线来<strong>预测未来</strong>。原来我们从初中时就可以预测未来了。：）</p><h5><strong>为什么我们可以用类似一条直线来预测呢？</strong>#####</h5><p>这个问题也可以换个说法，<strong>在什么情况下可以使用线性回归算法？</strong></p><blockquote><ol><li>看数据的分布是有一定规律的，可以通过直线或曲线来拟合数据的中心。</li><li>需要预测的变量是连续的值，比如房价，股票价格。而不是离散值，比如只有男、女等。</li></ol></blockquote><p>再来看看多参数的<strong>线性回归模型</strong>：$$h_\theta(x^i)=\theta_0 x_0^i+\theta_1 x_1^i+\theta_2 x_2^i \cdots+\theta_n x_n^i$$用<strong>向量表示</strong>：$$h_\theta(x)=\theta_0\begin{bmatrix} x_0^1 \x_0^2\ \vdots \x_0^n \end{bmatrix}+\theta_1\begin{bmatrix} x_1^1 \x_1^2\ \vdots \x_1^n \end{bmatrix}+\cdots+\theta_1\begin{bmatrix} x_1^1 \x_1^2\ \vdots \x_1^n \end{bmatrix}$$用<strong>矩阵</strong>表示：$$H=\begin{bmatrix}x_0^1 &amp; x_1^1 &amp; x_2^1  &amp; \cdots &amp; x_n^1 \x_0^2 &amp; x_1^2 &amp; x_2^2  &amp; \cdots &amp; x_n^2 \x_0^3 &amp; x_1^3 &amp; x_2^3  &amp; \cdots &amp; x_n^3 \\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\x_0^n &amp; x_1^n &amp; x_2^n  &amp; \cdots &amp; x_n^n \\end{bmatrix}*\begin{bmatrix}\theta_0 \\theta_1 \\theta_2 \\vdots \\theta_n \\end{bmatrix}=X\theta$$简不简单？有了这个公式，我们就能<strong>预测未来</strong>了：）</p><hr><h4><strong>2. 如何预测？</strong>####</h4><p>上面那个模型中$x$是确定的，即我们的各种特征数据，不确定的是$\theta$值。只要找到了$\theta$，我们就可以写出那个模型方程式，再把新的数据代入到$x$中，就知道了<strong>未来</strong>。所以，如何算出$\theta$?</p><p>针对一元回归模型，不同的$\theta$意味这不同的斜率，不同的斜率他们和真实数据的拟合程度是不一样的。如何确定$\theta$使得预测的误差最小呢？$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h\theta(x^i)-y^i)^2$$</p><p>其中$y^i$表示真实数据，而$h_\theta(x^i)$表示预测数据。<strong>说人话：$\theta$要满足这样的条件，即预测出来各个点的值与真实值之间的差的平方和最小。</strong></p><p>现在的问题就转换成了<strong>求$J(\theta)$的最小值</strong>问题了！即：$$\min_{\theta_1,\theta_2 \cdots\theta_n}J(\theta_1,\theta_2,\cdots,\theta_n)$$这个问题有两种解决方案：</p><h5><strong>1. 梯度下降</strong>#####</h5><p>要了解梯度下降算法，首先要知道求导公式的意义：$$\frac{\partial J}{\partial \theta}=\frac{\Delta y}{\Delta x}=tg \alpha$$<strong>说人话：每变化一点点$\theta$，随之而变的$J$变化了多少？</strong>还是没懂！？ 上一个百度图片：</p><blockquote><p><img src="http://img.blog.csdn.net/20170914173743114?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">图片中$\Delta x$就是变化的那一点点（对应$\partial \theta$或$\Delta y$），而对应的曲线$f(x)$，发生了$\Delta y$这么多变化。当N和M非常接近时（即$\Delta x$很小很小），我们可以用PQ的高度（$dy$）来近似NM的高度（其高度$dy$=在$x_0$处的斜率 * $\Delta x$）。</p></blockquote><p>知道了这个我们再来看看<strong>梯度下降算法公式</strong>：$$\theta_j := \theta_j - \alpha\frac{\partial J}{\partial\theta_j} \tag{*}$$</p><p>$\frac{\partial J}{\partial\theta_j}$ 可以简单的理解为上图中的$tga$，$\alpha$是个正常数，学名叫<strong>学习速率</strong>。这个$tga$很神奇，在小于90°，它是个正实数；在大于90°时是个负实数。</p><p>所以，对于梯度下降算法公式，当T倾斜向上，即角度小于90°时，$\alpha \frac{\partial J}{\partial\theta_j}$值为正，$\theta_j$从大变小，$\frac{\partial J}{\partial\theta_j}$不断的趋近于0，$\theta_j$不断的向左移动减小，直到移动到曲线的底部；<img src="http://img.blog.csdn.net/20170914183926463?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">当T倾斜向下时，即角度大于90°时，$\alpha  \frac{\partial J}{\partial\theta_j}$值为负，$\theta_j$从小变大，$\frac{\partial J}{\partial\theta_j}$不断的趋近于0，$\theta_j$不断的向右移动增大，直到移动到曲线的底部<img src="http://img.blog.csdn.net/20170914184358503?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p><p>理解了原理，下面就是如何算$\frac{\partial J}{\partial\theta_j}$了：$$\frac{\partial J}{\partial\theta_j}=\partial\frac{\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2 }{\partial\theta_j}$$</p><p>把$ \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2 $展开：$$J=\frac{1}{2m}[(h_\theta(x^1)-y^1)^2+h_\theta(x^2)-y^2)^2+\cdots+h_\theta(x^m)-y^m)^2]$$<strong>注意：上面的$x^1$表示第一个训练样本，而$y^1$是第一个训练样本所对应的真实目标值。</strong>我们对其中一个子式子继续展开研究：$$\frac{\partial(h_\theta(x^1)-y^1)^2}{\partial\theta_j}=\frac{\partial[(\theta_0x_0^1+\theta_1x_1^1+\cdots+\theta_nx_n^1)-y^1]^2}{\partial\theta_j}$$$$=2((\theta_0x_0^1+\theta_1x_1^1+\cdots+\theta_nx_n^1)-y^1)x_j^1$$</p><p>简化成向量的形式：$$\frac{\partial(h_\theta(x^1)-y^1)^2}{\partial\theta_j}=2( \begin{bmatrix} x_0^1&amp;x_1^1&amp;\cdots&amp;x_n^1\end{bmatrix} \begin{bmatrix} \theta_0\ \theta_1 \ \vdots \ \theta_n\end{bmatrix} - y^1)x_j^1==2(X^1\theta-y^1)  x_j^1$$所以最终我们的梯度下降$\theta​$参数的确认式子为:$$\theta_j := \theta_j - \alpha \frac{1}{2m} \sum_{i=1}^m(2(X^i\theta-y^i)x_j^i)=\theta_j- \alpha\frac{1}{m}\sum_{i=1}^m((X^i\theta-y^i)x_j^i)$$$$(j=0,1,...,n)$$</p><p>对求和公式展开后写成矩阵的形式：$$\theta:=\theta-\alpha \frac{1}{m}((X^1\theta - y^1)\begin{bmatrix}x_0^1 \x_1^1 \\vdots \x_n^1\end{bmatrix}+(X^2\theta - y^2)\begin{bmatrix}x_0^2 \x_1^2 \\vdots \x_n^2\end{bmatrix}+\cdots\+(X^n\theta - y^n)\begin{bmatrix}x_0^n \x_1^n \\vdots \x_n^n\end{bmatrix})<br>(其中\theta为列向量)$$</p><p>$$\theta:=\theta-\alpha \frac{1}{m}\begin{bmatrix}x_0^1 &amp; x_0^2 &amp; \cdots &amp; x_0^n\x_1^1 &amp; x_1^2 &amp; \cdots &amp; x_1^n\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \x_n^1 &amp; x_n^2 &amp;  \cdots &amp; x_n^n\end{bmatrix}\begin{bmatrix}X^1\theta-y^1\X^2\theta-y^2\\vdots \X^n\theta-y^n\end{bmatrix}$$<strong>终极公式：</strong></p><blockquote><p>$$\theta:=\theta-\alpha \frac{1}{m}(X^T(X\theta-Y)) \tag{*}$$什么意思？意思是是只要不断的迭代$\theta$，最终$\alpha\frac{1}{m}(X^T(X\theta-Y))$会收敛到0，从而得到一个收敛后的$\theta$，获得最小值。</p></blockquote><h5><strong>2. 正规方程</strong>#####</h5><p>正规方程可以更快速简单的求解$\theta$值，再一起推导一下。首先正规方程也是从这个方程而来：$$ J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2=\frac{1}{2m}[(h_\theta(x^1)-y^1)^2+h_\theta(x^2)-y^2)^2+\cdots+h_\theta(x^m)-y^m)^2]$$因为$X^TX=X^2$所以$$J(\theta)=\frac{1}{2m}[(h_\theta(x^1)-y^1)^T(h_\theta(x^1)-y^1)+\cdots+(h_\theta(x^n)-y^n)^T(h_\theta(x^n)-y^n)]$$</p><p>$$=\frac{1}{2m}\begin{bmatrix}(h_\theta(x^1)-y^1)^T \(h_\theta(x^2)-y^2)^T \\vdots \(h_\theta(x^n)-y^n)^T \end{bmatrix}\begin{bmatrix}(h_\theta(x^1)-y^1) &amp;(h_\theta(x^2)-y^2) &amp;\cdots &amp;(h_\theta(x^n)-y^n)\end{bmatrix}$$</p><p>$$=\frac{1}{2m}(X\theta-y)^T(X\theta-y)^=\frac{1}{2m}[((X\theta)^T-y^T)(X\theta-y)]$$</p><p>$$=\frac{1}{2m}[\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty]$$</p><p>对$\theta$求导，且求导后的值要趋于0，所以：$$\frac{\partial J}{\partial \theta}=\frac{\partial(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^2)}{\partial\theta}=0$$</p><p>因为$\theta^TX^Ty=y^TX\theta$所以有：$$\frac{\partial(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^2)}{\partial\theta}=X^TX\theta-2X^Ty=0$$</p><p>$$(X^TX)^{-1}(X^TX)\theta=(X^TX)^{-1}X^Ty$$$$\theta=(X^TX)^{-1}X^Ty \tag{*}$$</p><p><strong>梯度方法和正规方程方法比较：</strong></p><table><thead><tr><th style="text-align:left">梯度下降方法</th><th style="text-align:left">正规方程</th></tr></thead><tbody><tr><td style="text-align:left">适合特征大于1W的情况</td><td style="text-align:left">适合特征小于1W的情况</td></tr><tr><td style="text-align:left">需要归一化（特征标准化）</td><td style="text-align:left">不需要归一化</td></tr><tr><td style="text-align:left">方法相对复杂</td><td style="text-align:left">方法简单</td></tr></tbody></table><h5>** 注：以上数学推导过程若有不严谨之处，欢迎指出！**#####</h5>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;先搞清楚什么是机器学习，这对定位问题是否应该使用机器学习来解决很重要，有些问题完全没必要使用机器学习，就没必要杀鸡用牛刀了。&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;&lt;strong&gt;什么是机器学习？&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Arthur Samuel(195
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习第二周总结</title>
    <link href="http://yoursite.com/2017/09/25/ML-Logistic%20Regression/"/>
    <id>http://yoursite.com/2017/09/25/ML-Logistic Regression/</id>
    <published>2017-09-25T10:31:11.911Z</published>
    <updated>2017-09-25T10:31:11.892Z</updated>
    
    <content type="html"><![CDATA[<h1>机器学习第二周总结——逻辑回归</h1><p>逻辑回归主要用于解决分类问题。如：这个外星人是男还是女。但有个前提，也是监督学习的特点，即训练样本中目标列是已经有值的。</p><p>由于目标列的值只有“是”（用1表示）和“否”（用0来表示），因此可以用类似log的曲线</p><blockquote><p>$$g(z)=\frac{1}{1+e^{-z}}$$</p></blockquote><p><img src="C:%5CUsers%5Cadmin%5CDesktop%5CQQ%E6%88%AA%E5%9B%BE20170922090424.jpg" alt="image"></p><p>作为我们的预测模型。这个模型的取值范围是[0,1]，当z&gt;=0时，该模型的值更靠近1；而当z&lt;0时,模拟值更靠近0.因此可以这样理解，当z&gt;=0时，目标值为“是”的可能性更大；而当z&lt;0时，目标值为“否”的可能性更大。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;机器学习第二周总结——逻辑回归&lt;/h1&gt;
&lt;p&gt;逻辑回归主要用于解决分类问题。如：这个外星人是男还是女。但有个前提，也是监督学习的特点，即训练样本中目标列是已经有值的。&lt;/p&gt;
&lt;p&gt;由于目标列的值只有“是”（用1表示）和“否”（用0来表示），因此可以用类似log的曲线&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/09/13/hello-world/"/>
    <id>http://yoursite.com/2017/09/13/hello-world/</id>
    <published>2017-09-13T09:03:46.929Z</published>
    <updated>2017-09-13T09:03:46.929Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2>Quick Start</h2><h3>Create a new post</h3><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure></p><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3>Run server</h3><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure></p><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3>Generate static files</h3><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure></p><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3>Deploy to remote sites</h3><p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure></p><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
