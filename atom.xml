<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JohnWey&#39;s Blog</title>
  
  <subtitle>终身学习</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-09-26T08:23:57.703Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Wei</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习第一周总结</title>
    <link href="http://yoursite.com/2017/09/26/ML-Linear%20Regression/"/>
    <id>http://yoursite.com/2017/09/26/ML-Linear Regression/</id>
    <published>2017-09-26T08:23:57.703Z</published>
    <updated>2017-09-26T08:23:57.703Z</updated>
    
    <content type="html"><![CDATA[<p>先搞清楚什么是机器学习，这对定位问题是否应该使用机器学习来解决很重要，有些问题完全没必要使用机器学习，就没必要杀鸡用牛刀了。</p><hr><h4 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a><strong>什么是机器学习？</strong></h4><ul><li><p><em>Arthur Samuel(1959):</em></p><blockquote><p>Field of study that gives computers the ability to learn without being explicitly programmed.<br><strong>说人话：不需要太多的编程就能使计算机拥有学习某一领域的能力。</strong>  </p></blockquote></li><li><p><em>Tom Mitchell(1998):</em></p><blockquote><p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.<br><strong>说人话：AlphaGo为了完成人机对战（下棋 T），不断学习下棋，从中获取经验（E），目的是为了提高和人比赛的胜率（P）。</strong></p></blockquote></li></ul><hr><h4 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a><strong>机器学习分类</strong></h4><p>机器学习涉及的范围很广，针对不同的问题，学习策略也很多，但总体而言大致可以分为<strong>监督学习</strong>和<strong>非监督学习</strong>两种（还有半监督学习和强化学习）。不同的问题需要使用不同的策略。</p><p>还是把这两种策略的含义搞清楚先：</p><p>#####<strong>什么是监督学习？</strong>#####</p><blockquote><p>可以理解为<strong>一部分数据的答案已经知道了</strong>。比如我们要预测未来大盘的点位，在历史中大盘的点位已经知道了；再比如我们要让机器知道给它的图片是个帅哥还是美女，前提是我们已经知道了这个图片是帅哥还是美女。</p></blockquote><p>可想而知，如果需要人工去分类（打标），是一个多大的工程。这也孕育出了很多以出售打标数据盈利的公司。</p><p>#####<strong>什么是无监督学习？</strong>#####</p><blockquote><p>和监督学习相反，<strong>数据的答案事先我们不知道</strong>。而寻找答案是一个无中生有的过程。比如你遇到一群外星人，这群外星人各自有着不同的特征，你需要通过聚类的方法把有相同特征的外星人分组到一起，然后研究他们哪些对人类友好，哪些对人类有威胁，这叫无监督学习。</p></blockquote><p><em>算法一览：</em></p><table><thead><tr><th style="text-align:left">机器学习种类</th><th style="text-align:left">算法分类</th><th style="text-align:left">算法</th></tr></thead><tbody><tr><td style="text-align:left">监督学习</td><td style="text-align:left">分类、 回归</td><td style="text-align:left">K近邻、朴素贝叶斯、决策树、随机森林、GBDT和支持向量机；     线性回归、逻辑回归</td></tr><tr><td style="text-align:left">无监督学习</td><td style="text-align:left">聚类、 推荐</td><td style="text-align:left">K-Means、DBSCAN、协同过滤</td></tr><tr><td style="text-align:left">半监督学习</td><td style="text-align:left">聚类、 推荐</td><td style="text-align:left">标签传播</td></tr><tr><td style="text-align:left">强化学习</td><td style="text-align:left"></td><td style="text-align:left">隐马尔可夫</td></tr></tbody></table><hr><p>####<strong>1. 能预测未来的神奇算法——线性回归</strong>####</p><p>说到线性回归，其实我们初中的时候就学过它的简单方程式，只不过那会儿我们没有安利这样一个高大上的名字，我们那会儿叫<strong>斜截公式</strong>：<br>$$<br>y = kx + b<br>$$<br>来看看只有一个参数的<strong>单参数线性回归模型</strong>：<br>$$<br>h_\theta(x^i)=\theta_0+\theta_1 x_1^i  (其中 x_0^i=1，x_1^i表示一个特征，这个特征有i个值 )<br>$$<br>所谓的特征就是二维表中具有计算意义的一列数据，比如</p><table><thead><tr><th style="text-align:left">ID</th><th style="text-align:left">Sex</th><th style="text-align:left">High</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:left">男</td><td style="text-align:left">170 cm</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left">女</td><td style="text-align:left">175 cm</td></tr><tr><td style="text-align:left">3</td><td style="text-align:left">男</td><td style="text-align:left">180 cm</td></tr><tr><td style="text-align:left">4</td><td style="text-align:left">女</td><td style="text-align:left">200 cm</td></tr></tbody></table><p>其中Sex是一个特征，High是另一个特征，ID不是个特征，它只是个序列索引而已。他们的i都是4，因为有4条数据。是不是秒懂？：） 是的，在一元线性回归算法中，我们就是用一条倾斜的直线来<strong>预测未来</strong>。原来我们从初中时就可以预测未来了。：）</p><p>#####<strong>为什么我们可以用类似一条直线来预测呢？</strong>#####<br>这个问题也可以换个说法，<strong>在什么情况下可以使用线性回归算法？</strong></p><blockquote><ol><li>看数据的分布是有一定规律的，可以通过直线或曲线来拟合数据的中心。</li><li>需要预测的变量是连续的值，比如房价，股票价格。而不是离散值，比如只有男、女等。</li></ol></blockquote><p>再来看看多参数的<strong>线性回归模型</strong>：<br>$$<br>h_\theta(x^i)=\theta_0 x_0^i+\theta_1 x_1^i+\theta_2 x_2^i \cdots+\theta_n x<em>n^i<br>$$<br>用<strong>向量表示</strong>：<br>$$<br>h</em>\theta(x)=\theta_0\begin{bmatrix} x_0^1 \x_0^2\ \vdots \x_0^n \end{bmatrix}<br>+\theta_1\begin{bmatrix} x_1^1 \x_1^2\ \vdots \x_1^n \end{bmatrix}<br>+\cdots<br>+\theta_1\begin{bmatrix} x_1^1 \x_1^2\ \vdots \x_1^n \end{bmatrix}<br>$$<br>用<strong>矩阵</strong>表示：<br>$$<br>H=\begin{bmatrix}<br>x_0^1 &amp; x_1^1 &amp; x_2^1  &amp; \cdots &amp; x_n^1 \<br>x_0^2 &amp; x_1^2 &amp; x_2^2  &amp; \cdots &amp; x_n^2 \<br>x_0^3 &amp; x_1^3 &amp; x_2^3  &amp; \cdots &amp; x_n^3 \<br>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\<br>x_0^n &amp; x_1^n &amp; x_2^n  &amp; \cdots &amp; x_n^n \<br>\end{bmatrix}<em><br>\begin{bmatrix}\theta_0 \<br>\theta_1 \<br>\theta_2 \<br>\vdots \<br>\theta_n \<br>\end{bmatrix}=X\theta<br>$$<br>简不简单？有了这个公式，我们就能<em>*预测未来</em></em>了：）</p><hr><p>####<strong>2. 如何预测？</strong>####<br>上面那个模型中$x$是确定的，即我们的各种特征数据，不确定的是$\theta$值。只要找到了$\theta$，我们就可以写出那个模型方程式，再把新的数据代入到$x$中，就知道了<strong>未来</strong>。所以，如何算出$\theta$?</p><p>针对一元回归模型，不同的$\theta$意味这不同的斜率，不同的斜率他们和真实数据的拟合程度是不一样的。如何确定$\theta$使得预测的误差最小呢？<br>$$<br>J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h\theta(x^i)-y^i)^2<br>$$</p><p>其中$y^i$表示真实数据，而$h_\theta(x^i)$表示预测数据。<br><strong>说人话：$\theta$要满足这样的条件，即预测出来各个点的值与真实值之间的差的平方和最小。</strong></p><p>现在的问题就转换成了<strong>求$J(\theta)$的最小值</strong>问题了！<br>即：<br>$$<br> \min_{\theta_1,\theta_2 \cdots\theta_n}J(\theta_1,\theta_2,\cdots,\theta_n)<br>$$<br>这个问题有两种解决方案：</p><h5 id="1-梯度下降"><a href="#1-梯度下降" class="headerlink" title="1. 梯度下降"></a><strong>1. 梯度下降</strong></h5><p>要了解梯度下降算法，首先要知道求导公式的意义：<br>$$<br> \frac{\partial J}{\partial \theta}=\frac{\Delta y}{\Delta x}=tg \alpha<br>$$<br><strong>说人话：每变化一点点$\theta$，随之而变的$J$变化了多少？</strong><br>还是没懂！？ 上一个百度图片：</p><blockquote><p><img src="http://img.blog.csdn.net/20170914173743114?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>图片中$\Delta x$就是变化的那一点点（对应$\partial \theta$或$\Delta y$），而对应的曲线$f(x)$，发生了$\Delta y$这么多变化。当N和M非常接近时（即$\Delta x$很小很小），我们可以用PQ的高度（$dy$）来近似NM的高度（其高度$dy$=在$x_0$处的斜率 * $\Delta x$）。</p></blockquote><p>知道了这个我们再来看看<strong>梯度下降算法公式</strong>：</p><blockquote><p>$$\theta_j := \theta_j - \alpha <em> \frac{\partial J}{\partial\theta_j}  \tag{</em>}$$</p></blockquote><p>$\frac{\partial J}{\partial\theta_j}$ 可以简单的理解为上图中的$tga$，$\alpha$是个正常数，学名叫<strong>学习速率</strong>。这个$tga$很神奇，在小于90°，它是个正实数；在大于90°时是个负实数。</p><p>所以，对于梯度下降算法公式，</p><blockquote><p>当T倾斜向上，即角度小于90°时，$\alpha <em> \frac{\partial J}{\partial\theta_j}$值为正，$\theta_j$从大变小，$\frac{\partial J}{\partial\theta_j}$不断的趋近于0，$\theta_j$不断的向左移动减小，直到移动到曲线的底部；<br><img src="http://img.blog.csdn.net/20170914183926463?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>当T倾斜向下时，即角度大于90°时，$\alpha </em> \frac{\partial J}{\partial\theta_j}$值为负，$\theta_j$从小变大，$\frac{\partial J}{\partial\theta_j}$不断的趋近于0，$\theta_j$不断的向右移动增大，直到移动到曲线的底部<br><img src="http://img.blog.csdn.net/20170914184358503?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p></blockquote><p>理解了原理，下面就是如何算$\frac{\partial J}{\partial\theta_j}$了：<br>$$<br>\frac{\partial J}{\partial\theta<em>j}=\partial\frac{\frac{1}{2m}\sum</em>{i=1}^m(h_\theta(x^i)-y^i)^2 }{\partial\theta_j}<br>$$</p><p>把$\frac{1}{2m}\sum<em>{i=1}^m(h</em>\theta(x^i)-y^i)^2$展开：<br>$$<br>J=\frac{1}{2m}[(h<em>\theta(x^1)-y^1)^2+h</em>\theta(x^2)-y^2)^2+\cdots+h<em>\theta(x^m)-y^m)^2]<br>$$<br><strong>注意：上面的$x^1$表示第一个训练样本，而$y^1$是第一个训练样本所对应的真实目标值。</strong><br>我们对其中一个子式子继续展开研究：<br>$$\frac{\partial(h</em>\theta(x^1)-y^1)^2}{\partial\theta_j}=\frac{\partial[(\theta_0x_0^1+\theta_1x_1^1+\cdots+\theta_nx_n^1)-y^1]^2}{\partial\theta_j}=2((\theta_0x_0^1+\theta_1x_1^1+\cdots+\theta_nx_n^1)-y^1)<em>x<em>j^1$$<br>简化成向量的形式：<br>$$<br>\frac{\partial(h</em>\theta(x^1)-y^1)^2}{\partial\theta_j}=2( \begin{bmatrix} x_0^1&amp;x_1^1&amp;\cdots&amp;x_n^1\end{bmatrix} \begin{bmatrix} \theta_0\ \theta_1 \ \vdots \ \theta_n\end{bmatrix} - y^1)</em>x_j^1==2(X^1\theta-y^1) <em> x_j^1<br>$$<br>所以最终我们的梯度下降$\theta$参数的确认式子为:<br>$$<br>\theta_j := \theta<em>j - \alpha \frac{1}{2m} \sum</em>{i=1}^m(2(X^i\theta-y^i)</em>x_j^i)<br>=\theta<em>j- \alpha\frac{1}{m}\sum</em>{i=1}^m((X^i\theta-y^i)*x_j^i)  \tag{j=0,1,…,n}<br>$$<br>对求和公式展开后写成矩阵的形式：<br>$$<br>\theta:=\theta-\alpha \frac{1}{m}((X^1\theta - y^1)<br>\begin{bmatrix}<br>x_0^1 \<br>x_1^1 \<br>\vdots \<br>x_n^1<br>\end{bmatrix}<br>+(X^2\theta - y^2)<br>\begin{bmatrix}<br>x_0^2 \<br>x_1^2 \<br>\vdots \<br>x_n^2<br>\end{bmatrix}<br>+\cdots\<br>+(X^n\theta - y^n)<br>\begin{bmatrix}<br>x_0^n \<br>x_1^n \<br>\vdots \<br>x_n^n<br>\end{bmatrix})<br>(其中\theta为列向量)<br>$$</p><p>$$<br>\theta:=\theta-\alpha \frac{1}{m}<br>\begin{bmatrix}<br>x_0^1 &amp; x_0^2 &amp; \cdots &amp; x_0^n\<br>x_1^1 &amp; x_1^2 &amp; \cdots &amp; x_1^n\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>x_n^1 &amp; x_n^2 &amp;  \cdots &amp; x_n^n<br>\end{bmatrix}<br>\begin{bmatrix}<br>X^1\theta-y^1\<br>X^2\theta-y^2\<br>\vdots \<br>X^n\theta-y^n<br> \end{bmatrix}<br>$$<br><strong>终极公式：</strong></p><blockquote><p>$$\theta:=\theta-\alpha \frac{1}{m}(X^T(X\theta-Y)) \tag{<em>}$$<br>什么意思？意思是是只要不断的迭代$\theta$，最终$\alpha\frac{1}{m}(X^T</em>(X\theta-Y))$会收敛到0，从而得到一个收敛后的$\theta$，获得最小值。</p></blockquote><h5 id="2-正规方程"><a href="#2-正规方程" class="headerlink" title="2. 正规方程"></a><strong>2. 正规方程</strong></h5><p>正规方程可以更快速简单的求解$\theta$值，再一起推导一下。<br>首先正规方程也是从这个方程而来：<br>$$ J(\theta)=\frac{1}{2m}\sum<em>{i=1}^m(h</em>\theta(x^i)-y^i)^2=\frac{1}{2m}[(h<em>\theta(x^1)-y^1)^2+h</em>\theta(x^2)-y^2)^2+\cdots+h<em>\theta(x^m)-y^m)^2]$$<br>因为<br>$X^TX=X^2$<br>所以<br>$$<br>J(\theta)=\frac{1}{2m}[(h</em>\theta(x^1)-y^1)^T(h<em>\theta(x^1)-y^1)+\cdots+(h</em>\theta(x^n)-y^n)^T(h_\theta(x^n)-y^n)]<br>$$</p><p>$$<br>=\frac{1}{2m}<br>\begin{bmatrix}<br>(h<em>\theta(x^1)-y^1)^T \<br>(h</em>\theta(x^2)-y^2)^T \<br>\vdots \<br>(h<em>\theta(x^n)-y^n)^T \end{bmatrix}<br>\begin{bmatrix}<br>(h</em>\theta(x^1)-y^1) &amp;<br>(h<em>\theta(x^2)-y^2) &amp;<br>\cdots &amp;<br>(h</em>\theta(x^n)-y^n)<br>\end{bmatrix}<br>$$</p><p>$$<br>=\frac{1}{2m}(X\theta-y)^T(X\theta-y)^=\frac{1}{2m}[((X\theta)^T-y^T)(X\theta-y)]=\frac{1}{2m}[\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty]<br>$$</p><p>对$\theta$求导，且求导后的值要趋于0，所以：<br>$$<br>\frac{\partial J}{\partial \theta}=\frac{\partial(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^2)}{\partial\theta}=0<br>$$</p><p>因为$\theta^TX^Ty=y^TX\theta$所以有：<br>$$<br>\frac{\partial(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^2)}{\partial\theta}=X^TX\theta-2X^Ty=0<br>$$</p><p>$$<br>(X^TX)^{-1}(X^TX)\theta=(X^TX)^{-1}X^Ty<br>$$</p><p>$$<br>\theta=(X^TX)^{-1}X^Ty \tag{*}<br>$$</p><p><strong>梯度方法和正规方程方法比较：</strong><br>| 梯度下降方法       | 正规方程        |<br>| :———– | :———- |<br>| 适合特征大于1W的情况  | 适合特征小于1W的情况 |<br>| 需要归一化（特征标准化） | 不需要归一化      |<br>| 方法相对复杂       | 方法简单        |</p><p>#####<strong> 注：以上数学推导过程若有不严谨之处，欢迎指出！</strong>#####</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;先搞清楚什么是机器学习，这对定位问题是否应该使用机器学习来解决很重要，有些问题完全没必要使用机器学习，就没必要杀鸡用牛刀了。&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;什么是机器学习？&quot;&gt;&lt;a href=&quot;#什么是机器学习？&quot; class=&quot;headerlink&quot; title=&quot;什
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习第二周总结</title>
    <link href="http://yoursite.com/2017/09/25/ML-Logistic%20Regression/"/>
    <id>http://yoursite.com/2017/09/25/ML-Logistic Regression/</id>
    <published>2017-09-25T10:31:11.911Z</published>
    <updated>2017-09-25T10:31:11.892Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习第二周总结——逻辑回归"><a href="#机器学习第二周总结——逻辑回归" class="headerlink" title="机器学习第二周总结——逻辑回归"></a>机器学习第二周总结——逻辑回归</h1><p>逻辑回归主要用于解决分类问题。如：这个外星人是男还是女。但有个前提，也是监督学习的特点，即训练样本中目标列是已经有值的。</p><p>由于目标列的值只有“是”（用1表示）和“否”（用0来表示），因此可以用类似log的曲线</p><blockquote><p> $$g(z)=\frac{1}{1+e^{-z}}$$</p></blockquote><p><img src="C:\Users\admin\Desktop\QQ截图20170922090424.jpg" alt="image"></p><p>作为我们的预测模型。这个模型的取值范围是[0,1]，当z&gt;=0时，该模型的值更靠近1；而当z<0时,模拟值更靠近0.因此可以这样理解，当z>=0时，目标值为“是”的可能性更大；而当z&lt;0时，目标值为“否”的可能性更大。</0时,模拟值更靠近0.因此可以这样理解，当z></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习第二周总结——逻辑回归&quot;&gt;&lt;a href=&quot;#机器学习第二周总结——逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;机器学习第二周总结——逻辑回归&quot;&gt;&lt;/a&gt;机器学习第二周总结——逻辑回归&lt;/h1&gt;&lt;p&gt;逻辑回归主要用于解决分类问题。如：这
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/09/13/hello-world/"/>
    <id>http://yoursite.com/2017/09/13/hello-world/</id>
    <published>2017-09-13T09:03:46.929Z</published>
    <updated>2017-09-13T09:03:46.929Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
