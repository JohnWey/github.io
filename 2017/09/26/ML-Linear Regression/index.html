<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="先搞清楚什么是机器学习，这对定位问题是否应该使用机器学习来解决很重要，有些问题完全没必要使用机器学习，就没必要杀鸡用牛刀了。  什么是机器学习？  Arthur Samuel(1959):   Field of study that gives computers the ability to learn without being explicitly programmed. 说人话：不需要太多">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习第一周总结">
<meta property="og:url" content="http://yoursite.com/2017/09/26/ML-Linear Regression/index.html">
<meta property="og:site_name" content="JohnWey&#39;s Blog">
<meta property="og:description" content="先搞清楚什么是机器学习，这对定位问题是否应该使用机器学习来解决很重要，有些问题完全没必要使用机器学习，就没必要杀鸡用牛刀了。  什么是机器学习？  Arthur Samuel(1959):   Field of study that gives computers the ability to learn without being explicitly programmed. 说人话：不需要太多">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://img.blog.csdn.net/20170914173743114?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170914183926463?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170914184358503?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:updated_time" content="2017-09-26T09:24:30.104Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习第一周总结">
<meta name="twitter:description" content="先搞清楚什么是机器学习，这对定位问题是否应该使用机器学习来解决很重要，有些问题完全没必要使用机器学习，就没必要杀鸡用牛刀了。  什么是机器学习？  Arthur Samuel(1959):   Field of study that gives computers the ability to learn without being explicitly programmed. 说人话：不需要太多">
<meta name="twitter:image" content="http://img.blog.csdn.net/20170914173743114?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/26/ML-Linear Regression/"/>





  <title>机器学习第一周总结 | JohnWey's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">JohnWey's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">终身学习</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/26/ML-Linear Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Wei">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JohnWey's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习第一周总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-26T09:24:30+00:00">
                2017-09-26
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/09/26/ML-Linear Regression/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/09/26/ML-Linear Regression/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>先搞清楚什么是机器学习，这对定位问题是否应该使用机器学习来解决很重要，有些问题完全没必要使用机器学习，就没必要杀鸡用牛刀了。</p>
<hr>
<h4><strong>什么是机器学习？</strong></h4>
<ul>
<li><em>Arthur Samuel(1959):</em></li>
</ul>
<blockquote>
<p>Field of study that gives computers the ability to learn without being explicitly programmed.
<strong>说人话：不需要太多的编程就能使计算机拥有学习某一领域的能力。</strong></p>
</blockquote>
<ul>
<li><em>Tom Mitchell(1998):</em></li>
</ul>
<blockquote>
<p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
<strong>说人话：AlphaGo为了完成人机对战（下棋 T），不断学习下棋，从中获取经验（E），目的是为了提高和人比赛的胜率（P）。</strong></p>
</blockquote>
<hr>
<h4><strong>机器学习分类</strong></h4>
<p>机器学习涉及的范围很广，针对不同的问题，学习策略也很多，但总体而言大致可以分为<strong>监督学习</strong>和<strong>非监督学习</strong>两种（还有半监督学习和强化学习）。不同的问题需要使用不同的策略。</p>
<p>还是把这两种策略的含义搞清楚先：</p>
<h5><strong>什么是监督学习？</strong>#####</h5>
<blockquote>
<p>可以理解为<strong>一部分数据的答案已经知道了</strong>。比如我们要预测未来大盘的点位，在历史中大盘的点位已经知道了；再比如我们要让机器知道给它的图片是个帅哥还是美女，前提是我们已经知道了这个图片是帅哥还是美女。</p>
</blockquote>
<p>可想而知，如果需要人工去分类（打标），是一个多大的工程。这也孕育出了很多以出售打标数据盈利的公司。</p>
<h5><strong>什么是无监督学习？</strong>#####</h5>
<blockquote>
<p>和监督学习相反，<strong>数据的答案事先我们不知道</strong>。而寻找答案是一个无中生有的过程。比如你遇到一群外星人，这群外星人各自有着不同的特征，你需要通过聚类的方法把有相同特征的外星人分组到一起，然后研究他们哪些对人类友好，哪些对人类有威胁，这叫无监督学习。</p>
</blockquote>
<p><em>算法一览：</em></p>
<table>
<thead>
<tr>
<th style="text-align:left">机器学习种类</th>
<th style="text-align:left">算法分类</th>
<th style="text-align:left">算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">监督学习</td>
<td style="text-align:left">分类、 回归</td>
<td style="text-align:left">K近邻、朴素贝叶斯、决策树、随机森林、GBDT和支持向量机；     线性回归、逻辑回归</td>
</tr>
<tr>
<td style="text-align:left">无监督学习</td>
<td style="text-align:left">聚类、 推荐</td>
<td style="text-align:left">K-Means、DBSCAN、协同过滤</td>
</tr>
<tr>
<td style="text-align:left">半监督学习</td>
<td style="text-align:left">聚类、 推荐</td>
<td style="text-align:left">标签传播</td>
</tr>
<tr>
<td style="text-align:left">强化学习</td>
<td style="text-align:left"></td>
<td style="text-align:left">隐马尔可夫</td>
</tr>
</tbody>
</table>
<hr>
<h4><strong>1. 能预测未来的神奇算法——线性回归</strong>####</h4>
<p>说到线性回归，其实我们初中的时候就学过它的简单方程式，只不过那会儿我们没有安利这样一个高大上的名字，我们那会儿叫<strong>斜截公式</strong>：
$$
y = kx + b
$$
来看看只有一个参数的<strong>单参数线性回归模型</strong>：
$$
h_\theta(x^i)=\theta_0+\theta_1 x_1^i  (其中 x_0^i=1，x_1^i表示一个特征，这个特征有i个值 )
$$
所谓的特征就是二维表中具有计算意义的一列数据，比如</p>
<table>
<thead>
<tr>
<th style="text-align:left">ID</th>
<th style="text-align:left">Sex</th>
<th style="text-align:left">High</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">男</td>
<td style="text-align:left">170 cm</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">女</td>
<td style="text-align:left">175 cm</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">男</td>
<td style="text-align:left">180 cm</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">女</td>
<td style="text-align:left">200 cm</td>
</tr>
</tbody>
</table>
<p>其中Sex是一个特征，High是另一个特征，ID不是个特征，它只是个序列索引而已。他们的i都是4，因为有4条数据。是不是秒懂？：） 是的，在一元线性回归算法中，我们就是用一条倾斜的直线来<strong>预测未来</strong>。原来我们从初中时就可以预测未来了。：）</p>
<h5><strong>为什么我们可以用类似一条直线来预测呢？</strong>#####</h5>
<p>这个问题也可以换个说法，<strong>在什么情况下可以使用线性回归算法？</strong></p>
<blockquote>
<ol>
<li>看数据的分布是有一定规律的，可以通过直线或曲线来拟合数据的中心。</li>
<li>需要预测的变量是连续的值，比如房价，股票价格。而不是离散值，比如只有男、女等。</li>
</ol>
</blockquote>
<p>再来看看多参数的<strong>线性回归模型</strong>：
$$
h_\theta(x^i)=\theta_0 x_0^i+\theta_1 x_1^i+\theta_2 x_2^i \cdots+\theta_n x_n^i
$$
用<strong>向量表示</strong>：
$$
h_\theta(x)=\theta_0\begin{bmatrix} x_0^1 \x_0^2\ \vdots \x_0^n \end{bmatrix}
+\theta_1\begin{bmatrix} x_1^1 \x_1^2\ \vdots \x_1^n \end{bmatrix}
+\cdots
+\theta_1\begin{bmatrix} x_1^1 \x_1^2\ \vdots \x_1^n \end{bmatrix}
$$
用<strong>矩阵</strong>表示：
$$
H=\begin{bmatrix}
x_0^1 &amp; x_1^1 &amp; x_2^1  &amp; \cdots &amp; x_n^1 \
x_0^2 &amp; x_1^2 &amp; x_2^2  &amp; \cdots &amp; x_n^2 \
x_0^3 &amp; x_1^3 &amp; x_2^3  &amp; \cdots &amp; x_n^3 \
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\
x_0^n &amp; x_1^n &amp; x_2^n  &amp; \cdots &amp; x_n^n \
\end{bmatrix}*
\begin{bmatrix}\theta_0 \
\theta_1 \
\theta_2 \
\vdots \
\theta_n \
\end{bmatrix}=X\theta
$$
简不简单？有了这个公式，我们就能<strong>预测未来</strong>了：）</p>
<hr>
<h4><strong>2. 如何预测？</strong>####</h4>
<p>上面那个模型中$x$是确定的，即我们的各种特征数据，不确定的是$\theta$值。只要找到了$\theta$，我们就可以写出那个模型方程式，再把新的数据代入到$x$中，就知道了<strong>未来</strong>。所以，如何算出$\theta$?</p>
<p>针对一元回归模型，不同的$\theta$意味这不同的斜率，不同的斜率他们和真实数据的拟合程度是不一样的。如何确定$\theta$使得预测的误差最小呢？
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h\theta(x^i)-y^i)^2
$$</p>
<p>其中$y^i$表示真实数据，而$h_\theta(x^i)$表示预测数据。
<strong>说人话：$\theta$要满足这样的条件，即预测出来各个点的值与真实值之间的差的平方和最小。</strong></p>
<p>现在的问题就转换成了<strong>求$J(\theta)$的最小值</strong>问题了！
即：
$$
\min_{\theta_1,\theta_2 \cdots\theta_n}J(\theta_1,\theta_2,\cdots,\theta_n)
$$
这个问题有两种解决方案：</p>
<h5><strong>1. 梯度下降</strong>#####</h5>
<p>要了解梯度下降算法，首先要知道求导公式的意义：
$$
\frac{\partial J}{\partial \theta}=\frac{\Delta y}{\Delta x}=tg \alpha
$$
<strong>说人话：每变化一点点$\theta$，随之而变的$J$变化了多少？</strong>
还是没懂！？ 上一个百度图片：</p>
<blockquote>
<p><img src="http://img.blog.csdn.net/20170914173743114?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">
图片中$\Delta x$就是变化的那一点点（对应$\partial \theta$或$\Delta y$），而对应的曲线$f(x)$，发生了$\Delta y$这么多变化。当N和M非常接近时（即$\Delta x$很小很小），我们可以用PQ的高度（$dy$）来近似NM的高度（其高度$dy$=在$x_0$处的斜率 * $\Delta x$）。</p>
</blockquote>
<p>知道了这个我们再来看看<strong>梯度下降算法公式</strong>：
$$
\theta_j := \theta_j - \alpha\frac{\partial J}{\partial\theta_j} \tag{*}
$$</p>
<p>$\frac{\partial J}{\partial\theta_j}$ 可以简单的理解为上图中的$tga$，$\alpha$是个正常数，学名叫<strong>学习速率</strong>。这个$tga$很神奇，在小于90°，它是个正实数；在大于90°时是个负实数。</p>
<p>所以，对于梯度下降算法公式，
当T倾斜向上，即角度小于90°时，$\alpha \frac{\partial J}{\partial\theta_j}$值为正，$\theta_j$从大变小，$\frac{\partial J}{\partial\theta_j}$不断的趋近于0，$\theta_j$不断的向左移动减小，直到移动到曲线的底部；
<img src="http://img.blog.csdn.net/20170914183926463?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述">
当T倾斜向下时，即角度大于90°时，$\alpha  \frac{\partial J}{\partial\theta_j}$值为负，$\theta_j$从小变大，$\frac{\partial J}{\partial\theta_j}$不断的趋近于0，$\theta_j$不断的向右移动增大，直到移动到曲线的底部
<img src="http://img.blog.csdn.net/20170914184358503?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSm9obldleQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>理解了原理，下面就是如何算$\frac{\partial J}{\partial\theta_j}$了：
$$
\frac{\partial J}{\partial\theta_j}=\partial\frac{\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2 }{\partial\theta_j}
$$</p>
<p>把$ \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2 $展开：
$$
J=\frac{1}{2m}[(h_\theta(x^1)-y^1)^2+h_\theta(x^2)-y^2)^2+\cdots+h_\theta(x^m)-y^m)^2]
$$
<strong>注意：上面的$x^1$表示第一个训练样本，而$y^1$是第一个训练样本所对应的真实目标值。</strong>
我们对其中一个子式子继续展开研究：
$$\frac{\partial(h_\theta(x^1)-y^1)^2}{\partial\theta_j}=\frac{\partial[(\theta_0x_0^1+\theta_1x_1^1+\cdots+\theta_nx_n^1)-y^1]^2}{\partial\theta_j}$$
$$
=2((\theta_0x_0^1+\theta_1x_1^1+\cdots+\theta_nx_n^1)-y^1)x_j^1
$$</p>
<p>简化成向量的形式：
$$
\frac{\partial(h_\theta(x^1)-y^1)^2}{\partial\theta_j}=2( \begin{bmatrix} x_0^1&amp;x_1^1&amp;\cdots&amp;x_n^1\end{bmatrix} \begin{bmatrix} \theta_0\ \theta_1 \ \vdots \ \theta_n\end{bmatrix} - y^1)x_j^1==2(X^1\theta-y^1)  x_j^1
$$
所以最终我们的梯度下降$\theta​$参数的确认式子为:
$$
\theta_j := \theta_j - \alpha \frac{1}{2m} \sum_{i=1}^m(2(X^i\theta-y^i)x_j^i)
=\theta_j- \alpha\frac{1}{m}\sum_{i=1}^m((X^i\theta-y^i)x_j^i)
$$
$$
(j=0,1,...,n)
$$</p>
<p>对求和公式展开后写成矩阵的形式：
$$
\theta:=\theta-\alpha \frac{1}{m}((X^1\theta - y^1)
\begin{bmatrix}
x_0^1 \
x_1^1 \
\vdots \
x_n^1
\end{bmatrix}
+(X^2\theta - y^2)
\begin{bmatrix}
x_0^2 \
x_1^2 \
\vdots \
x_n^2
\end{bmatrix}
+\cdots\
+(X^n\theta - y^n)
\begin{bmatrix}
x_0^n \
x_1^n \
\vdots \
x_n^n
\end{bmatrix})<br>
(其中\theta为列向量)
$$</p>
<p>$$
\theta:=\theta-\alpha \frac{1}{m}
\begin{bmatrix}
x_0^1 &amp; x_0^2 &amp; \cdots &amp; x_0^n\
x_1^1 &amp; x_1^2 &amp; \cdots &amp; x_1^n\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
x_n^1 &amp; x_n^2 &amp;  \cdots &amp; x_n^n
\end{bmatrix}
\begin{bmatrix}
X^1\theta-y^1\
X^2\theta-y^2\
\vdots \
X^n\theta-y^n
\end{bmatrix}
$$
<strong>终极公式：</strong></p>
<blockquote>
<p>$$\theta:=\theta-\alpha \frac{1}{m}(X^T(X\theta-Y)) \tag{*}$$
什么意思？意思是是只要不断的迭代$\theta$，最终$\alpha\frac{1}{m}(X^T(X\theta-Y))$会收敛到0，从而得到一个收敛后的$\theta$，获得最小值。</p>
</blockquote>
<h5><strong>2. 正规方程</strong>#####</h5>
<p>正规方程可以更快速简单的求解$\theta$值，再一起推导一下。
首先正规方程也是从这个方程而来：
$$ J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2=\frac{1}{2m}[(h_\theta(x^1)-y^1)^2+h_\theta(x^2)-y^2)^2+\cdots+h_\theta(x^m)-y^m)^2]$$
因为
$X^TX=X^2$
所以
$$
J(\theta)=\frac{1}{2m}[(h_\theta(x^1)-y^1)^T(h_\theta(x^1)-y^1)+\cdots+(h_\theta(x^n)-y^n)^T(h_\theta(x^n)-y^n)]
$$</p>
<p>$$
=\frac{1}{2m}
\begin{bmatrix}
(h_\theta(x^1)-y^1)^T \
(h_\theta(x^2)-y^2)^T \
\vdots \
(h_\theta(x^n)-y^n)^T \end{bmatrix}
\begin{bmatrix}
(h_\theta(x^1)-y^1) &amp;
(h_\theta(x^2)-y^2) &amp;
\cdots &amp;
(h_\theta(x^n)-y^n)
\end{bmatrix}
$$</p>
<p>$$
=\frac{1}{2m}(X\theta-y)^T(X\theta-y)^=\frac{1}{2m}[((X\theta)^T-y^T)(X\theta-y)]
$$</p>
<p>$$
=\frac{1}{2m}[\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty]
$$</p>
<p>对$\theta$求导，且求导后的值要趋于0，所以：
$$
\frac{\partial J}{\partial \theta}=\frac{\partial(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^2)}{\partial\theta}=0
$$</p>
<p>因为$\theta^TX^Ty=y^TX\theta$所以有：
$$
\frac{\partial(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^2)}{\partial\theta}=X^TX\theta-2X^Ty=0
$$</p>
<p>$$
(X^TX)^{-1}(X^TX)\theta=(X^TX)^{-1}X^Ty
$$
$$
\theta=(X^TX)^{-1}X^Ty \tag{*}
$$</p>
<p><strong>梯度方法和正规方程方法比较：</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">梯度下降方法</th>
<th style="text-align:left">正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">适合特征大于1W的情况</td>
<td style="text-align:left">适合特征小于1W的情况</td>
</tr>
<tr>
<td style="text-align:left">需要归一化（特征标准化）</td>
<td style="text-align:left">不需要归一化</td>
</tr>
<tr>
<td style="text-align:left">方法相对复杂</td>
<td style="text-align:left">方法简单</td>
</tr>
</tbody>
</table>
<h5>** 注：以上数学推导过程若有不严谨之处，欢迎指出！**#####</h5>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/25/ML-Logistic Regression/" rel="next" title="机器学习第二周总结">
                <i class="fa fa-chevron-left"></i> 机器学习第二周总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          
            <p class="site-author-name" itemprop="name">Wei</p>
            <p class="site-description motion-element" itemprop="description"></p>
        </div>

        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-number">1.</span> <span class="nav-text">什么是机器学习？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-number">2.</span> <span class="nav-text">机器学习分类</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#undefined"><span class="nav-number">2.1.</span> <span class="nav-text">什么是监督学习？#####</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#undefined"><span class="nav-number">2.2.</span> <span class="nav-text">什么是无监督学习？#####</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-number">3.</span> <span class="nav-text">1. 能预测未来的神奇算法——线性回归####</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#undefined"><span class="nav-number">3.1.</span> <span class="nav-text">为什么我们可以用类似一条直线来预测呢？#####</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#undefined"><span class="nav-number">4.</span> <span class="nav-text">2. 如何预测？####</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#undefined"><span class="nav-number">4.1.</span> <span class="nav-text">1. 梯度下降#####</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#undefined"><span class="nav-number">4.2.</span> <span class="nav-text">2. 正规方程#####</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#undefined"><span class="nav-number">4.3.</span> <span class="nav-text">** 注：以上数学推导过程若有不严谨之处，欢迎指出！**#####</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wei</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.2</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>


  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://keeplearning365.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2017/09/26/ML-Linear Regression/';
          this.page.identifier = '2017/09/26/ML-Linear Regression/';
          this.page.title = '机器学习第一周总结';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://keeplearning365.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  





  

  

  

  
  


  

  

</body>
</html>
